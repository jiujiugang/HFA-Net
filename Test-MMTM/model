import torch
import torch.nn as nn
import torch.nn.functional as F


def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0.0, std=0.02)
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)


class MMTM(nn.Module):
    def __init__(self, dim_visual, dim_skeleton, ratio=4):
        super(MMTM, self).__init__()

        dim = dim_visual + dim_skeleton  # 输入通道数：视觉和骨骼模态
        dim_out = int(2 * dim / ratio)  # 融合后特征通道数

        self.fc_squeeze = nn.Linear(dim, dim_out)  # 融合后的压缩层

        # 分别为视觉和骨骼模态定义全连接层
        self.fc_visual = nn.Linear(dim_out, dim_visual)
        self.fc_skeleton = nn.Linear(dim_out, dim_skeleton)

        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()

        # 初始化权重
        with torch.no_grad():
            self.fc_squeeze.apply(init_weights)
            self.fc_visual.apply(init_weights)
            self.fc_skeleton.apply(init_weights)

    def forward(self, visual, skeleton):
        # 将输入的视觉和骨骼特征按通道展平并平均
        squeeze_array = []
        for tensor in [visual, skeleton]:
            tview = tensor.view(tensor.shape[:2] + (-1,))  # Flatten the spatial dims
            squeeze_array.append(torch.mean(tview, dim=-1))  # 平均化空间维度
        squeeze = torch.cat(squeeze_array, 1)  # 拼接视觉和骨骼特征

        # 通过fc_squeeze层生成注意力因子
        excitation = self.fc_squeeze(squeeze)
        excitation = self.relu(excitation)

        # 分别对视觉和骨骼特征进行加权
        vis_out = self.fc_visual(excitation)
        sk_out = self.fc_skeleton(excitation)

        # 使用sigmoid得到加权因子
        vis_out = self.sigmoid(vis_out)
        sk_out = self.sigmoid(sk_out)

        # 如果输入是3D图像，将其还原为正确的维度
        dim_diff = len(visual.shape) - len(vis_out.shape)
        vis_out = vis_out.view(vis_out.shape + (1,) * dim_diff)  # 维度匹配
        dim_diff = len(skeleton.shape) - len(sk_out.shape)
        sk_out = sk_out.view(sk_out.shape + (1,) * dim_diff)

        # 将视觉和骨骼特征分别与其加权因子相乘
        return visual * vis_out, skeleton * sk_out


class MMTNet(nn.Module):
    def __init__(self, num_classes=4):
        super(MMTNet, self).__init__()

        # 添加视觉和ECG编码器，将图像转为高维特征
        self.visual_encoder = nn.Sequential(
            nn.Conv2d(3, 512, kernel_size=3, stride=2, padding=1),  # [B, 3, 224, 224] -> [B, 512, 112, 112]
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))  # [B, 512, 112, 112] -> [B, 512, 1, 1]
        )

        self.ecg_encoder = nn.Sequential(
            nn.Conv2d(3, 128, kernel_size=3, stride=2, padding=1),  # [B, 3, 224, 224] -> [B, 128, 112, 112]
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))  # -> [B, 128, 1, 1]
        )

        self.mmtm = MMTM(dim_visual=512, dim_skeleton=128, ratio=4)
        self.fc = nn.Linear(512 + 128, num_classes)

    def forward(self, visual_img, ecg_img):
        vis_feat = self.visual_encoder(visual_img)  # [B, 512, 1, 1]
        ecg_feat = self.ecg_encoder(ecg_img)        # [B, 128, 1, 1]

        vis_feat, ecg_feat = self.mmtm(vis_feat, ecg_feat)  # MMTM expects shape [B, C, H, W]

        # 拼接两个模态
        combined = torch.cat([vis_feat.view(vis_feat.size(0), -1),  # [B, 512]
                              ecg_feat.view(ecg_feat.size(0), -1)], dim=1)  # [B, 128]
        out = self.fc(combined)  # [B, num_classes]
        return out

